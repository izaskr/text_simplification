
## Where the samples will be written
save_data: /home/AK/skrjanec/text_simplification/vocabs
## Where the vocab(s) will be written
src_vocab: /home/AK/skrjanec/text_simplification/vocabs/shared_all_ct.txt
# sh_set_vocab.src
#tgt_vocab: /home/CE/skrjanec/chart_onmt/vocabs/sh_delex_vocab.tgt # if the vocab is shared, commeted out
# Prevent overwriting existing files in the folder
#overwrite: False

# Corpus opts:
data:
    corpus_1:
        path_src: /home/AK/skrjanec/text_simplification/data/apa/source_target_copy/source_train_level_lev_bin.txt
        path_tgt: /home/AK/skrjanec/text_simplification/data/apa/source_target_copy/target_train.txt
    valid:
        path_src: /home/AK/skrjanec/text_simplification/data/apa/source_target_copy/source_val_level_lev_bin.txt
        path_tgt: /home/AK/skrjanec/text_simplification/data/apa/source_target_copy/target_val.txt

share_vocab: True


# Train on a single GPU
world_size: 1
gpu_ranks: [0]
seed: 1234

# model setup
encoder_type: brnn
decoder_type: rnn
src_word_vec_size: 512
tgt_word_vec_size: 512
enc_layers: 2
dec_layers: 2
enc_rnn_size: 1024
dec_rnn_size: 1024
rnn_type: LSTM
# global_attention (dot, general, mlp)
global_attention: general
copy_attn: True

# optimization
optim: adam
learning_rate: 0.0007 # default: 0.001, larger LRs worsen the performance
dropout: 0.3
batch_size: 64


# Where to save the checkpoints
save_model:  /home/AK/skrjanec/text_simplification/models/8/rnn_basic_copy
save_checkpoint_steps: 5
train_steps: 450
valid_steps: 5
